{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/belalabusaleh/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en\n",
    "\n",
    "def warn(*args, **kwargs): #remove warnings\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "thursday = re.sub(r'Chapter \\d+', '', thursday)\n",
    "sense = re.sub(r'CHAPTER .*', '', sense)\n",
    "    \n",
    "thursday = text_cleaner(thursday[:int(len(thursday)/5)])\n",
    "sense = text_cleaner(sense[:int(len(sense)/5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "thur_doc = nlp(thursday)\n",
    "sense_doc = nlp(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(To, Edmund, Clerihew, Bentley, A, cloud, was,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Science, announced, nonentity, and, art, admi...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Like, the, white, lock, of, Whistler, ,, that...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Life, was, a, fly, that, faded, ,, and, death...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(They, twisted, even, decent, sin, to, shapes,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0           1\n",
       "0  (To, Edmund, Clerihew, Bentley, A, cloud, was,...  Chesterton\n",
       "1  (Science, announced, nonentity, and, art, admi...  Chesterton\n",
       "2  (Like, the, white, lock, of, Whistler, ,, that...  Chesterton\n",
       "3  (Life, was, a, fly, that, faded, ,, and, death...  Chesterton\n",
       "4  (They, twisted, even, decent, sin, to, shapes,...  Chesterton"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "thur_sent = [[sent, \"Chesterton\"] for sent in thur_doc.sents]\n",
    "sense_sent = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(thur_sent + sense_sent)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "thurwords = bag_of_words(thur_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(thurwords + sensewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n",
      "Processing row 1000\n",
      "Processing row 1050\n",
      "Processing row 1100\n",
      "Processing row 1150\n",
      "Processing row 1200\n",
      "Processing row 1250\n",
      "Processing row 1300\n",
      "Processing row 1350\n",
      "Processing row 1400\n",
      "Processing row 1450\n",
      "Processing row 1500\n",
      "Processing row 1550\n",
      "Processing row 1600\n",
      "Processing row 1650\n",
      "Processing row 1700\n",
      "Processing row 1750\n",
      "Processing row 1800\n",
      "Processing row 1850\n",
      "Processing row 1900\n",
      "Processing row 1950\n",
      "Processing row 2000\n",
      "Processing row 2050\n",
      "Processing row 2100\n",
      "Processing row 2150\n",
      "Processing row 2200\n",
      "Processing row 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flicker</th>\n",
       "      <th>admittance</th>\n",
       "      <th>indelicacy</th>\n",
       "      <th>impressive</th>\n",
       "      <th>sincerely</th>\n",
       "      <th>overcome</th>\n",
       "      <th>arrive</th>\n",
       "      <th>unanswerable</th>\n",
       "      <th>knock</th>\n",
       "      <th>Christmas</th>\n",
       "      <th>...</th>\n",
       "      <th>inheritor</th>\n",
       "      <th>friendship</th>\n",
       "      <th>blast</th>\n",
       "      <th>conservative</th>\n",
       "      <th>despotism</th>\n",
       "      <th>accusation</th>\n",
       "      <th>applaud</th>\n",
       "      <th>threaten</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(To, Edmund, Clerihew, Bentley, A, cloud, was,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Science, announced, nonentity, and, art, admi...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Like, the, white, lock, of, Whistler, ,, that...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Life, was, a, fly, that, faded, ,, and, death...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(They, twisted, even, decent, sin, to, shapes,...</td>\n",
       "      <td>Chesterton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  flicker admittance indelicacy impressive sincerely overcome arrive  \\\n",
       "0       0          0          0          0         0        0      0   \n",
       "1       0          0          0          0         0        0      0   \n",
       "2       0          0          0          0         0        0      0   \n",
       "3       0          0          0          0         0        0      0   \n",
       "4       0          0          0          0         0        0      0   \n",
       "\n",
       "  unanswerable knock Christmas     ...     inheritor friendship blast  \\\n",
       "0            0     0         0     ...             0          0     0   \n",
       "1            0     0         0     ...             0          0     0   \n",
       "2            0     0         0     ...             0          0     0   \n",
       "3            0     0         0     ...             0          0     0   \n",
       "4            0     0         0     ...             0          0     0   \n",
       "\n",
       "  conservative despotism accusation applaud threaten  \\\n",
       "0            0         0          0       0        0   \n",
       "1            0         0          0       0        0   \n",
       "2            0         0          0       0        0   \n",
       "3            0         0          0       0        0   \n",
       "4            0         0          0       0        0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (To, Edmund, Clerihew, Bentley, A, cloud, was,...  Chesterton  \n",
       "1  (Science, announced, nonentity, and, art, admi...  Chesterton  \n",
       "2  (Like, the, white, lock, of, Whistler, ,, that...  Chesterton  \n",
       "3  (Life, was, a, fly, that, faded, ,, and, death...  Chesterton  \n",
       "4  (They, twisted, even, decent, sin, to, shapes,...  Chesterton  \n",
       "\n",
       "[5 rows x 3216 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.25,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8885613207547169\n",
      "Test set score: 0.7773851590106007\n",
      "\n",
      "Cross Val Forest Classifier: 0.76 (+/- 0.02)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.75      0.91      0.82       321\n",
      "  Chesterton       0.83      0.61      0.70       245\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       566\n",
      "   macro avg       0.79      0.76      0.76       566\n",
      "weighted avg       0.79      0.78      0.77       566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(x_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(x_train, y_train))\n",
    "print('Test set score:', rfc.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(rfc, x_train, y_train, cv=5)\n",
    "print(\"\\nCross Val Forest Classifier: %0.2f (+/- %0.2f)\\n\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "y_pred = rfc.predict(x_test) #predict y based on x_test\n",
    "print(classification_report(y_test, y_pred)) #true value vs predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1696, 3214) (1696,)\n",
      "Training set score: 0.8856132075471698\n",
      "Test set score: 0.823321554770318\n",
      "\n",
      "Cross Val Logistic Regression: 0.78 (+/- 0.02)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.79      0.94      0.86       321\n",
      "  Chesterton       0.89      0.67      0.77       245\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       566\n",
      "   macro avg       0.84      0.81      0.81       566\n",
      "weighted avg       0.83      0.82      0.82       566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(x_train, y_train)\n",
    "\n",
    "print('Training set score:', lr.score(x_train, y_train))\n",
    "print('Test set score:', lr.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(lr, x_train, y_train, cv=5)\n",
    "print(\"\\nCross Val Logistic Regression: %0.2f (+/- %0.2f)\\n\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "y_pred = lr.predict(x_test) #predict y based on x_test\n",
    "print(classification_report(y_test, y_pred)) #true value vs predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.5607311320754716\n",
      "Test set score: 0.5388692579505301\n",
      "\n",
      "Cross Val GradientBoosting: 0.71 (+/- 0.03)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.56      0.85      0.68       321\n",
      "  Chesterton       0.40      0.13      0.19       245\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       566\n",
      "   macro avg       0.48      0.49      0.43       566\n",
      "weighted avg       0.49      0.54      0.47       566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(x_train, y_train))\n",
    "print('Test set score:', clf.score(x_test, y_test))\n",
    "\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=5)\n",
    "print(\"\\nCross Val GradientBoosting: %0.2f (+/- %0.2f)\\n\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "y_pred = clf.predict(x_test) #predict y based on x_test\n",
    "print(classification_report(y_test, y_pred)) #true value vs predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Bag of words works best here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try tf-idf now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/belalabusaleh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/belalabusaleh/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "['[ Sense and Sensibility by Jane Austen 1811 ]', 'CHAPTER 1', 'The family of Dashwood had long been settled in Sussex .', 'By a former marriage , Mr . Henry Dashwood had one son : by his present lady , three daughters .']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#reading in the data, this time in the form of paragraphs\n",
    "sense_1=gutenberg.paras('austen-sense.txt')\n",
    "#processing\n",
    "sense_paras=[]\n",
    "for paragraph in sense_1:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    sense_paras.append(' '.join(para))\n",
    "\n",
    "print(sense_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1311\n",
      "Original sentence: \" Nor I ,\" answered Marianne with energy , \" our situations then are alike .\n",
      "Tf_idf vector: {'lucy': 0.27039798753420413, 'wished': 0.34952461452398426, 'pressed': 0.44840296631250753, 'reason': 0.36540692802784613, 'happiness': 0.33450540357651476, 'talk': 0.3619037298085233, 'obliged': 0.34952461452398426, 'elinor': 0.32418266162976966}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(sense_paras,\n",
    "                                   test_size=0.25)\n",
    "\n",
    "#Set our parameters\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sense_paras_tfidf=vectorizer.fit_transform(sense_paras)\n",
    "print(\"Number of features: %d\" % sense_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sense_paras_tfidf, \n",
    "                                              test_size=0.25)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now we have our vectors, with one vector per paragraph.\n",
    "It's time to do some dimension reduction. We use the Singular Value Decomposition (SVD) function from sklearn rather than PCA because we don't want to mean-center our variables (and thus lose sparsity):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 65.28344609780035\n",
      "Component 0:\n",
      "\" Being very sure I have long lost your affections , I have thought myself at liberty to bestow my own on another , and have no doubt of being as happy with him as I once used to think I might be with you ; but I scorn to accept a hand while the heart was another ' s .                                                                                                                                       1.0\n",
      "Mrs . Dashwood , not less watchful of what passed than her daughter , but with a mind very differently influenced , and therefore watching to very different effect , saw nothing in the Colonel ' s behaviour but what arose from the most simple and self - evident sensations , while in the actions and words of Marianne she persuaded herself to think that something more than gratitude already dawned .    1.0\n",
      "\" And you will never see me otherwise .                                                                                                                                                                                                                                                                                                                                                                             1.0\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "This event , while it raised the spirits of Elinor , restored to those of her sister all , and more than all , their former agitation .    0.669088\n",
      "\" I cannot agree with you there ,\" said Elinor .                                                                                           0.599967\n",
      "\" Civil ! Did you see nothing but only civility ? I saw a vast deal more .                                                                 0.599967\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Have you been lately in Sussex ?\"                 0.983119\n",
      "But so it was .                                     0.983119\n",
      "\" How long has this been known to you , Elinor ?    0.983119\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "This event , while it raised the spirits of Elinor , restored to those of her sister all , and more than all , their former agitation .                    0.590804\n",
      "They were both silent for a few moments .                                                                                                                  0.562401\n",
      "\" Margaret ,\" said Marianne with great warmth , \" you know that all this is an invention of your own , and that there is no such person in existence .\"    0.524593\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" Shall you be in town this winter , Miss Dashwood ?\"                                                                                                 0.660346\n",
      "\" Upon the knoll behind the house .                                                                                                                   0.660346\n",
      "Marianne was surprised and confused , yet she could not help smiling at the quiet archness of his manner , and after a moment ' s silence , said ,    0.660346\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer. We are going to reduce the feature space from 1942 to 130.\n",
    "svd= TruncatedSVD(250)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 86.16843205471466\n",
      "Component 0:\n",
      "\" The Colonel is a ninny , my dear ; because he has two thousand a - year himself , he thinks that nobody else can marry on less .    1.0\n",
      "\" Well , Marianne ,\" said Elinor , as soon as he had left them , \" for ONE morning I think you have done pretty well .                1.0\n",
      "\" He would certainly have done more justice to simple and elegant prose .                                                             1.0\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" What a charming thing it is that Mrs . Dashwood can spare you both for so long a time together !\"     0.460554\n",
      "\" Come Colonel ,\" said Mrs . Jennings , \" before you go , do let us know what you are going about .\"    0.395159\n",
      "\" Oh !                                                                                                  0.392633\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Perhaps ,\" said Elinor , smiling , \" we may come to the same point .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.907644\n",
      "\" Hush !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.907644\n",
      "Elinor had often wished for an opportunity of attempting to weaken her mother ' s dependence on the attachment of Edward and herself , that the shock might be less when the whole truth were revealed , and now on this attack , though almost hopeless of success , she forced herself to begin her design by saying , as calmly as she could , \" I like Edward Ferrars very much , and shall always be glad to see him ; but as to the rest of the family , it is a matter of perfect indifference to me , whether I am ever known to them or not .\"    0.907644\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" Go to him , Elinor ,\" she cried , as soon as she could speak , \" and force him to come to me .                                                                                                                     0.717668\n",
      "His son was sent for as soon as his danger was known , and to him Mr . Dashwood recommended , with all the strength and urgency which illness could command , the interest of his mother - in - law and sisters .    0.717668\n",
      "\" Oh , yes ; he had been staying a fortnight with us .                                                                                                                                                               0.526865\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" We can mean no other ,\" cried Lucy , smiling .                                                                     0.533674\n",
      "Elinor ' s satisfaction , at the moment of removal , was more positive .                                             0.533674\n",
      "Her second note , which had been written on the morning after the dance at the Middletons ', was in these words :    0.452190\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Look at testing data\n",
    "\n",
    "# Run SVD on the testing data, then project the testing data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for fun lets use word2vec for word comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text[0:900000]\n",
    "\n",
    "\n",
    "# Import all the Austen in the Project Gutenberg corpus.\n",
    "austen = \"\"\n",
    "for novel in ['persuasion','emma','sense']:\n",
    "    work = gutenberg.raw('austen-' + novel + '.txt')\n",
    "    austen = austen + work\n",
    "\n",
    "# Clean the data.\n",
    "austen_clean = text_cleaner(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data. This can take some time.\n",
    "nlp = spacy.load('en')\n",
    "austen_doc = nlp(austen_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lady', 'russell', 'steady', 'age', 'character', 'extremely', 'provide', 'thought', 'second', 'marriage', 'need', 'apology', 'public', 'apt', 'unreasonably', 'discontent', 'woman', 'marry', 'sir', 'walter', 'continue', 'singleness', 'require', 'explanation']\n",
      "We have 9298 sentences and 900000 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Organize the parsed doc into sentences, while filtering out punctuation\n",
    "# and stop words, and converting words to lower case lemmas.\n",
    "sentences = []\n",
    "for sentence in austen_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower() #convert all to lowercase\n",
    "        for token in sentence\n",
    "        if not token.is_stop #remove stop words\n",
    "        and not token.is_punct #remove punctuation\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "\n",
    "print(sentences[20])\n",
    "print('We have {} sentences and {} tokens.'.format(len(sentences), len(austen_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel\n",
    "    min_count=5,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nineteen', 0.9058294296264648), ('hitherto', 0.9029128551483154), ('confident', 0.9012742042541504), ('suddenly', 0.9002410173416138), ('afternoon', 0.9000874757766724), ('jealousy', 0.8993202447891235), ('surprised', 0.8987928032875061), ('forth', 0.8986408710479736), ('suitable', 0.8968305587768555), ('intelligence', 0.8958185911178589)]\n",
      "0.75374824\n",
      "marriage\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "#woman is to lady as man is to ???: print result\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('lady', 'gentleman'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
